{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7364143e",
   "metadata": {},
   "source": [
    "# Chapter 5: Tool Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36725f11",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Hands-On Code Example (LangChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a428add",
   "metadata": {},
   "source": [
    "> Adapted and modified from https://docs.google.com/document/d/1bE4iMljhppqGY1p48gQWtZvk6MfRuJRCiba1yRykGNE/edit?tab=t.0\n",
    "> \n",
    "> Do  2 Okt 2025 16:00:42 BST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c322293",
   "metadata": {},
   "source": [
    "The implementation of tool use within the LangChain framework is a two-stage process. Initially, one or more tools are defined, typically by encapsulating existing Python functions or other runnable components. Subsequently, these tools are bound to a language model, thereby granting the model the capability to generate a structured tool-use request when it determines that an external function call is required to fulfill a user's query.\n",
    "\n",
    "The following implementation will demonstrate this principle by first defining a simple function to simulate an information retrieval tool. Following this, an agent will be constructed and configured to leverage this tool in response to user input. The execution of this example requires the installation of the core LangChain libraries and a model-specific provider package. Furthermore, proper authentication with the selected language model service, typically via an API key configured in the local environment, is a necessary prerequisite. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e0f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool as langchain_tool\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb29f69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Language model initialized: models/gemini-2.0-flash\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759418143.151321  672654 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# UNCOMMENT\n",
    "# Prompt the user securely and set API keys as an environment variables\n",
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "try:\n",
    "  # A model with function/tool calling capabilities is required.\n",
    "  llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
    "  print(f\"‚úÖ Language model initialized: {llm.model}\")\n",
    "except Exception as e:\n",
    "  print(f\"üõë Error initializing language model: {e}\")\n",
    "  llm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29434b7",
   "metadata": {},
   "source": [
    "## Note\n",
    "Alternatively, you can load environment variables from a .env file if present with: \n",
    "\n",
    "```python\n",
    "load_dotenv()  \n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db9759da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define a Tool ---\n",
    "@langchain_tool\n",
    "def search_information(query: str) -> str:\n",
    "  \"\"\"\n",
    "  Provides factual information on a given topic. Use this tool to find answers to phrases\n",
    "  like 'capital of France' or 'weather in London?'.\n",
    "  \"\"\"\n",
    "  print(f\"\\n--- üõ†Ô∏è Tool Called: search_information with query: '{query}' ---\")\n",
    "  # Simulate a search tool with a dictionary of predefined results.\n",
    "  simulated_results = {\n",
    "      \"weather in london\": \"The weather in London is currently cloudy with a temperature of 15¬∞C.\",\n",
    "      \"capital of france\": \"The capital of France is Paris.\",\n",
    "      \"population of earth\": \"The estimated population of Earth is around 8 billion people.\",\n",
    "      \"tallest mountain\": \"Mount Everest is the tallest mountain above sea level.\",\n",
    "      \"default\": f\"Simulated search result for '{query}': No specific information found, but the topic seems interesting.\"\n",
    "  }\n",
    "  result = simulated_results.get(query.lower(), simulated_results[\"default\"])\n",
    "  print(f\"--- TOOL RESULT: {result} ---\")\n",
    "  return result\n",
    "\n",
    "tools = [search_information]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de222442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create a Tool-Calling Agent ---\n",
    "if llm:\n",
    "  # This prompt template requires an `agent_scratchpad` placeholder for the agent's internal steps.\n",
    "  agent_prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", \"You are a helpful assistant.\"),\n",
    "      (\"human\", \"{input}\"),\n",
    "      (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "  ])\n",
    "\n",
    "  # Create the agent, binding the LLM, tools, and prompt together.\n",
    "  agent = create_tool_calling_agent(llm, tools, agent_prompt)\n",
    "\n",
    "  # AgentExecutor is the runtime that invokes the agent and executes the chosen tools.\n",
    "  # The 'tools' argument is not needed here as they are already bound to the agent.\n",
    "  agent_executor = AgentExecutor(agent=agent, verbose=True, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb446568",
   "metadata": {},
   "source": [
    "## Note\n",
    "When your agent runs, the {agent_scratchpad} might look like:\n",
    "\n",
    "```\n",
    "I need to search for information about the capital of France.\n",
    "\n",
    "Action: search_information\n",
    "Action Input: {\"query\": \"capital of France\"}\n",
    "Observation: The capital of France is Paris.\n",
    "\n",
    "Now I have the information needed to answer the user's question.\n",
    "```\n",
    "\n",
    "The {agent_scratchpad} starts empty for each new input/query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d5e10d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_agent_with_tool(query: str):\n",
    "  \"\"\"Invokes the agent executor with a query and prints the final response.\"\"\"\n",
    "  print(f\"\\n--- üèÉ Running Agent with Query: '{query}' ---\")\n",
    "  try:\n",
    "      response = await agent_executor.ainvoke({\"input\": query})\n",
    "      print(\"\\n--- ‚úÖ Final Agent Response ---\")\n",
    "      print(response[\"output\"])\n",
    "  except Exception as e:\n",
    "      print(f\"\\nüõë An error occurred during agent execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "635542e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "  \"\"\"Runs all agent queries concurrently.\"\"\"\n",
    "  tasks = [\n",
    "      run_agent_with_tool(\"What is the capital of France?\"),\n",
    "      run_agent_with_tool(\"What's the weather like in London?\"),\n",
    "      run_agent_with_tool(\"Tell me something about dogs.\") # Should trigger the default tool response\n",
    "  ]\n",
    "  await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d8ab98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1759418143.209016  672654 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üèÉ Running Agent with Query: 'What is the capital of France?' ---\n",
      "\n",
      "--- üèÉ Running Agent with Query: 'What's the weather like in London?' ---\n",
      "\n",
      "--- üèÉ Running Agent with Query: 'Tell me something about dogs.' ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_information` with `{'query': 'dogs'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "--- üõ†Ô∏è Tool Called: search_information with query: 'dogs' ---\n",
      "--- TOOL RESULT: Simulated search result for 'dogs': No specific information found, but the topic seems interesting. ---\n",
      "\u001b[36;1m\u001b[1;3mSimulated search result for 'dogs': No specific information found, but the topic seems interesting.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_information` with `{'query': 'weather in London?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "--- üõ†Ô∏è Tool Called: search_information with query: 'weather in London?' ---\n",
      "--- TOOL RESULT: Simulated search result for 'weather in London?': No specific information found, but the topic seems interesting. ---\n",
      "\u001b[36;1m\u001b[1;3mSimulated search result for 'weather in London?': No specific information found, but the topic seems interesting.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_information` with `{'query': 'capital of France'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "--- üõ†Ô∏è Tool Called: search_information with query: 'capital of France' ---\n",
      "--- TOOL RESULT: The capital of France is Paris. ---\n",
      "\u001b[36;1m\u001b[1;3mThe capital of France is Paris.\u001b[0m\u001b[32;1m\u001b[1;3mI'm sorry, I don't have any specific information about dogs. But they are interesting! Perhaps you could ask me a more specific question?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "--- ‚úÖ Final Agent Response ---\n",
      "I'm sorry, I don't have any specific information about dogs. But they are interesting! Perhaps you could ask me a more specific question?\n",
      "\n",
      "\u001b[32;1m\u001b[1;3mThe capital of France is Paris.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "--- ‚úÖ Final Agent Response ---\n",
      "The capital of France is Paris.\n",
      "\n",
      "\u001b[32;1m\u001b[1;3mI'm sorry, I don't have the current weather information for London.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "--- ‚úÖ Final Agent Response ---\n",
      "I'm sorry, I don't have the current weather information for London.\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2383b",
   "metadata": {},
   "source": [
    "## Note\n",
    "In modern Jupyter, you can also use top-level await: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256c44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üèÉ Running Agent with Query: 'What is the capital of France?' ---\n",
      "\n",
      "--- üèÉ Running Agent with Query: 'What's the weather like in London?' ---\n",
      "\n",
      "--- üèÉ Running Agent with Query: 'Tell me something about dogs.' ---\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_information` with `{'query': 'dogs'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "--- üõ†Ô∏è Tool Called: search_information with query: 'dogs' ---\n",
      "--- TOOL RESULT: Simulated search result for 'dogs': No specific information found, but the topic seems interesting. ---\n",
      "\u001b[36;1m\u001b[1;3mSimulated search result for 'dogs': No specific information found, but the topic seems interesting.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_information` with `{'query': 'weather in London?'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "--- üõ†Ô∏è Tool Called: search_information with query: 'weather in London?' ---\n",
      "--- TOOL RESULT: Simulated search result for 'weather in London?': No specific information found, but the topic seems interesting. ---\n",
      "\u001b[36;1m\u001b[1;3mSimulated search result for 'weather in London?': No specific information found, but the topic seems interesting.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_information` with `{'query': 'capital of France'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "--- üõ†Ô∏è Tool Called: search_information with query: 'capital of France' ---\n",
      "--- TOOL RESULT: The capital of France is Paris. ---\n",
      "\u001b[36;1m\u001b[1;3mThe capital of France is Paris.\u001b[0m\u001b[32;1m\u001b[1;3mThe capital of France is Paris.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "--- ‚úÖ Final Agent Response ---\n",
      "The capital of France is Paris.\n",
      "\n",
      "\u001b[32;1m\u001b[1;3mI'm sorry, I don't have any specific information about dogs. But they are interesting! Perhaps you could ask me a more specific question?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "--- ‚úÖ Final Agent Response ---\n",
      "I'm sorry, I don't have any specific information about dogs. But they are interesting! Perhaps you could ask me a more specific question?\n",
      "\n",
      "\u001b[32;1m\u001b[1;3mI'm sorry, I don't have the current weather information for London.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "--- ‚úÖ Final Agent Response ---\n",
      "I'm sorry, I don't have the current weather information for London.\n"
     ]
    }
   ],
   "source": [
    "await main()  # Now this works in Jupyter!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1b1994",
   "metadata": {},
   "source": [
    "## Note\n",
    "The agent finds an answer for 'What is the capital of France?', but no answer for 'What's the weather like in London?' and 'Tell me something about dogs.'. That might be a bit odd at first. \n",
    "\n",
    "What actually happened underneath is that the LLM rephrased the queries before invoking the tool. We can see that in the output lines: \n",
    "\n",
    "``` \n",
    "--- üõ†Ô∏è Tool Called: search_information with query: 'capital of France' ---\n",
    "--- üõ†Ô∏è Tool Called: search_information with query: 'weather in London?' ---\n",
    "--- üõ†Ô∏è Tool Called: search_information with query: 'dogs' ---\n",
    "```\n",
    "\n",
    "These are the actual queries, that are passed into the (function) tool `search_information`. And because the tool only modifies the queries to lowercase and then performs a strict matching dictionary search, we only get a correct answer for 'capital of france'. We don't get the correct answer (but the default answer) for 'weather in London?' because of the question mark at the end. And for 'dogs' we just get the default answer. \n",
    "\n",
    "Further, the LLM then modifies the answers from the tool to give a final answer. Here, an overview of what happened: \n",
    "\n",
    "| Original Query | LLM Re-phrased Query | lowercase | Tool Answer | LLM Re-phrased Answer |\n",
    "|---|---|---|---|---|\n",
    "| \"What is the capital of France?\" | 'capital of France' | 'capital of france' | \"The capital of France is Paris.\" | The capital of France is Paris. |\n",
    "| \"What's the weather like in London?\" | 'weather in London?' | 'weather in london?' | \"No specific information found, but the topic seems interesting.\" | I'm sorry, I don't have the current weather information for London. |\n",
    "| \"Tell me something about dogs.\"| 'dogs' | 'dogs' | \"No specific information found, but the topic seems interesting.\" | I'm sorry, I don't have any specific information about dogs. But they are interesting! Perhaps you could ask me a more specific question? | \n",
    "\n",
    "Feel free, to play around with the `search_information` code, e.g. adding \n",
    "\n",
    "```python \n",
    "query = query.lower().strip().rstrip('?')\n",
    "```\n",
    "\n",
    "or an new dictionary entry for \"dogs\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc3e98",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
